---
title: "MovieLens Project Submission"
author: "Kirtimay Pendse"
date: "6/9/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=6)
```

# Introduction

This project is part of the HarvardX: PH125.9x Data Science: Capstone course^[https://courses.edx.org/courses/course-v1:HarvardX+PH125.9x+1T2020/course/], the final course for the Data Science Professional Certificate. This project is centered around creating a movie recommendation algorithm using a subset of the MovieLens dataset. 

The algorithm was developed using a training set (referred to as the 'edx' set) and was tested on the 'validation' set; then, the random mean squared error (RMSE) was calculated to evaluate the proximity of the predictions generated by the algorithm to the true values in the validation set. This report is split into four sections: first, the objective and motivation behind the project is highlighted, then exploratory data analysis is conducted, following which the modeling approach to develop the predicted movie rating algorithm is presented. Finally, the modeling resutls are presented along with a discussion on the algorithm's performance and its limitations.

## Objective

The MovieLens dataset^[https://grouplens.org/datasets/movielens/latest/] is compiled by the GroupLens research group at the University of Minnesota. The dataset contains over 20 million ratings for more than 27,000 movies with around 140,000 users. The overall aim of this project is to develop an algorithm that can predict user ratings (ranging from 0.5 to 5) in the validation set using a subset of the MovieLens dataset provided by the edX staff (the 'edx' dataset).

The algorithm's performance is evaluated using the RMSE, a measure of the difference between the model's predicted values and the observed values which can be written as the following function^[Taken from Prof. Irizarry's "Introduction to Data Science" 33.7.3], where u represents a user, i represent a movie and N is the total number of user/movie combinations: 
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$

An 'accurate' model entails obtaining an RMSE of < 0.86490, and RMSE is the value used to calculcate the accuracy/effectiveness of the different models created. 

# Methods and Analysis

## Preparing the data

First, the dataset is downloaded from the MovieLens website and split into a training set (the 'edx' set) and a 'validation' set. The 'edx' dataset is further split into a train set and a test set, and is used to create the prediction algorithm. Once the desired RMSE is reached, the model is tested on the 'validation' dataset for the final analysis. 


```{r, data prep, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

#Loading required packages
library(lubridate)
if(!require(ggthemes)) 
  install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(scales)) 
  install.packages("scales", repos = "http://cran.us.r-project.org")
library(dplyr)
library(knitr)
library(ggplot2)
library(dslabs)
library(lubridate)

#The following code was given by the edX staff

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") #use set.seed(1) if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% #validation will be our test set
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed) #edx is training set

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

The 'edx' dataset was split into a train set (90% of the data in edx) and a test set (10% of the data in edx) using the following code-

```{r edx, echo=FALSE, warning=FALSE, message = FALSE, eval = TRUE}
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
temp <- edx[test_index,]

#To ensure userId and movieId in the test set are also in the train set
test_set <- temp %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

#Adding rows removed from the test set back into the train set
removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)
rm(test_index, temp, removed)
```

## Exploratory Analysis

For the initial data exploration, the head() function was used to get an overall 'feel' for the data.

```{r head, echo=FALSE}
head(edx) %>% print.data.frame() #to inspect the data briefly, and get a 'feel'
```

From the data frame above, it's observed that the dataset contains 6 variables, with each user and movie having a unique user ID and movie ID respectively, along with a rating assigned to each user-movie pair, and a timestamp. It's also evident that the title of each movie has the year in brackets, and multiple genres can be assigned to a movie. 

Then, the class for each variable was determined.

```{r sapply, echo=FALSE}
sapply(edx, class) #ascertain the class of each variable
```

It's observed that 'userId' and 'timestamp' are integers, 'movieId' and 'rating' are numeric, and 'title' and 'genres' are characters. 

A summary of the edx dataset was also determined:

```{r summary, echo=FALSE}
summary(edx) #get summary stats for the individual variables
```

The initial observations made from the table above are that the mean rating is around 3.5, indicating a possibility that most users tend to rate movies slightly higher, and that there doesn't seem to be any missing values.

## Univariate Plots

The following section inspects the variables in more detail through some plots.

### Date

From the plot below, it's evident that the movies with the most ratings came out in 2000 and then 2005. As expected, the movies released most recently tend to have fewer ratings, but there are a few years (such as 1997 and 2002) where movies have fewer ratings. 

```{r date, echo=FALSE} 
#in the line above, can add fig.height = ? or fig.width= ? to specify things
edx %>% mutate(year = year(as_datetime(timestamp, origin="1970-01-01"))) %>%
  ggplot(aes(x=year)) +
  geom_histogram(color = "black", bins = 30) + 
  ggtitle("Distribution of Ratings per Year") +
  xlab("Year") +
  ylab("# Ratings") +
  scale_y_continuous(labels = comma) +
  theme(plot.title = element_text(hjust = 0.5))
```


### Movies 

In the plot below, it can be seen that some movies are rated much more frequently than others, and that there's a few movies with as few as one rating. In the models created in subsequent sections, it's important to note that these low ratings could result in faulty estimates for the predictions made by the model.  

```{r movies, echo=FALSE}
edx %>% group_by(movieId) %>%
  summarise(n=n()) %>%
  ggplot(aes(n)) +
  geom_histogram(color = "black", bins = 30) +
  scale_x_log10() + 
  ggtitle("Distribution of Movies") +
  xlab("# Ratings") +
  ylab("# Movies") +
  theme(plot.title = element_text(hjust = 0.5))
```

### Users 

In the plot below, it's evident that most of the users have rated between 40 to 110 movies, and some users are very active and have rated in numbers close to a thousand. This highlights the importance of adding a user effects term in the models built in the next section.

```{r users, echo=FALSE}
edx %>% group_by(userId) %>%
  summarise(n=n()) %>%
  ggplot(aes(n)) +
  geom_histogram(color = "black", bins = 30) +
  scale_x_log10() + 
  ggtitle("Distribution of Users") +
  xlab("# Ratings") +
  ylab("# Users") + 
  scale_y_continuous(labels = comma) +
  theme(plot.title = element_text(hjust = 0.5))
```

### Ratings 

As highlighted in the summary statistics and the graph below, users tend to be more positive than critical while rating movies. 4, 3, and 5 (in that order) are the most common ratings given, and half point ratings are not as common as whole numbers. 0.5 is the rating least commonly assigned to a movie.

```{r ratings, echo=FALSE}
edx %>% group_by(rating) %>% 
  summarise(count=n()) %>%
  ggplot(aes(x=rating, y=count)) + 
  geom_line() +
  geom_point() +
  scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x))) +
  ggtitle("Distribution of Ratings") + 
  xlab("Rating") +
  ylab("Count") 
```

cheeky heatmap here maybe?

## Modeling

To build an efficient recomemndation system algorithm, the most important features that would help predict a user's rating need to be identified. This section starts off with a model that randomly predicts a user's rating based on the observed probabilities in the training subset of the edx dataset, followed by a simple model built around the mean of the observed values, following which three linear models are built- one accounting for user effects, one accounting for movie effects, and one accounting for both movie and user effects. Then, a regularization parameter is added to the movie and user effects model, and a final model using matrix factorization is built. As mentioned earlier, RMSE values are used to evaluate each model's performance.

The RMSE is calculcated using the following code:

```{r RMSE, echo = TRUE}
RMSE <- function(predicted_ratings, true_ratings){
  sqrt(mean((predicted_ratings - true_ratings)^2))
}
```

### Predicting Randomly

In this simple model, user ratings are randomly predicted using the observed probabilities in the training subset of the edx dataset. After calculcating the probability of each rating, the model predicts ratings for the test subset and the RMSE compares the predictions with the actual rating. This model should produce the worst RMSE of all models generated, and mostly serves as a stepping stone towards creating a more accurate model. This model uses a Monte Carlo simulation to approximate the rating distribution, since the real distribution of ratings for the entire population is unknown. 

```{r random, message=FALSE, warning=FALSE}
set.seed(43, sample.kind = "Rounding")

#To create a probability distribution of each rating
p <- function(x, y) mean(y == x)
rating <- seq(0.5,5,0.5)

#Using a MC simulation to estimate the probability
B <- 10000
M <- replicate(B, {
  z <- sample(train_set$rating, 100, replace = T)
  sapply(rating, p, y=z)
})
prob_1 <- sapply(1:nrow(M), function(x) mean(M[x,]))

#To randomly predict ratings
y_hat <- sample(rating, size = nrow(test_set), 
                       replace = TRUE, prob = prob_1)

#Store results in a table
results_table <- tibble('Method of Analysis' = "Desired RMSE", RMSE = 0.8649)
results_table <- bind_rows(results_table, 
                    tibble('Method of Analysis' = "Predicting Randomly", 
                           RMSE = RMSE(test_set$rating, y_hat)))
results_table
```

The RMSE of this model is extremely high, almost 1.5, and the following models would bring the value down closer to the desired RMSE. 

### Building Linear Models

1. The Average Rating Model
The first linear model is centered around the idea that users will ascribe the same rating to all movies, and that variation within movies is accounted in a randomly distributed error term. This can be written as the following equation:

$$\hat Y_{u,i}=\mu+\epsilon_{i,u}$$

$\hat Y$ represents the predicted user rating, $\mu$ represents the mean of the observed data (since the average minimizes RMSE, this model uses the predicted rating as the mean of observed ratings), and $\epsilon_{i,u}$ is an error term.

```{r}
#Calculating the mean of observed values in the train_set
mu <- mean(train_set$rating)

#Adding the results to the RMSE table  
results_table <- bind_rows(results_table, 
                    tibble('Method of Analysis'= "Using the Average Rating", 
                           RMSE = RMSE(test_set$rating, mu)))
results_table
```

Using $\mu$ as the prediction for user ratings significantly lowers the RMSE value from randomly predicting, but is still too large compared to the desired RMSE.

2. Including User Effects

Users tend to have different rating distributions; for instance, some users may like most of the movies they rate and consistently give them high ratings, whereas some users could be more critical and give movies lower ratings. The user effect ($b_u$) can be captured through this linear model:

$$\hat Y_{u,i}=\mu + b_u + \epsilon_{i,u}$$

```{r}
#Calculating bu (the user effect)
bu <- train_set %>% 
  left_join(bi, by = 'movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

#Predicting user rating with mean & bu  
y_hat_bu <- mu + test_set %>% 
  left_join(bu, by = "userId") %>% 
  .$b_u

#Add the results
results_table <- bind_rows(results_table, 
                    tibble('Method of Analysis'= "Including User Effect", 
                           RMSE = RMSE(test_set$rating, y_hat_bu)))
results_table
```

Including the user effect lowered the RMSE from 1.06 to 0.993.

3. Including Movie Effects.

Similar to the user effect, variability within movies can be explained through different rating distributions for different movies; intuitively speaking this makes sense, as some movies are very popular and garner huge public support, whereas some artsy, independent movies have limited support, and might have lower ratings due to being 'unpopular'. The following equation highlights a linear model accounting for the movie effect ($b_i$:

$$\hat Y_{u,i}=\mu + b_i + \epsilon_{i,u}$$

```{r}
#Calculating bi (the movie effect)
bi <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

#Predicting user rating with mean & bi  
y_hat_bi <- mu + test_set %>% 
  left_join(bi, by = "movieId") %>% 
  .$b_i

#Add the results
results_table <- bind_rows(results_table, 
                    tibble('Method of Analysis' = "Including Movie Effect", 
                           RMSE = RMSE(test_set$rating, y_hat_bi)))
results_table
```

Including the movie effect reduced the RMSE to 0.943.

4. Including User and Movie Effects

A more accurate model would account for both user and movie effects, and can be written as the following equation:

$$\hat Y_{u,i}=\mu+b_i+b_u+\epsilon_{u,i}$$ 

```{r}
#Predicting user rating with mu, bi and bu
y_hat_bi_bu <- test_set %>% 
  left_join(bi, by='movieId') %>%
  left_join(bu, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

#Add the results
results_table <- bind_rows(results_table, 
                    tibble('Method of Analysis' = "Including Movie and User Effects", 
                           RMSE = RMSE(test_set$rating, y_hat_bi_bu)))
results_table
```

Including both movie and user effects significantly lowered the RMSE to 0.865.

### Using Regularization

The linear models created above provided reasonable user rating predictions, but fail to account for the fact that many movies have a small number of ratings, and that some users have rated a very small number of movies. Having these small sample sizes for movie-user combinations can lead to large estimated errors, and so a penalty term should be added. Regularization, a form of regression that 'regularizes' (constrains) estimates to zero, allows for penalizing these small sample sizes and also helps in avoiding overfitting (creating an algorithm that functions extremely well on a training set but doesn't generalize well on additional data).

To regularize movie and user effects, a penalty term $\lambda$ is used; $\lambda$ can be thought of as a tuning parameter, and so can be ascribed a range of values from which the best value (the one that minimizes RMSE) will be picked. 


```{r lambdas, echo = TRUE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})
```

The optimal value of $\lambda$ is 5; this is the value that is used to shrink the estimates $b_{i}$ and $b_{u}$ in case of a small movie-user combination sample size.

```{r min_lambda, echo = TRUE}
lambda <- lambdas[which.min(rmses)]
lambda

#This can be confirmed via the following plot as well:
qplot(lambdas, rmses)  
```

$\lambda$ is then added to the movie and user effect linear model.

```{r}
#Predict a regularized estimate of y_hat
reg_y_hat <- test_set %>% 
  left_join(b_u, by = "userId") %>%
  left_join(b_i, by = "movieId") %>%
  mutate(pred = mu + b_u + b_i) %>%
  pull(pred)
# Update the result table
results_table <- bind_rows(results_table, 
                    tibble('Method of Analysis' = "Regularization", 
                           RMSE = RMSE(test_set$rating, reg_y_hat)))
results_table
```

### Matrix Factorization

## Final Validation

```{r final_valid, message=FALSE, warning=FALSE}
mu_edx <- mean(edx$rating)
# Movie effect (bi)
b_i_edx <- edx %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu_edx)/(n()+lambda))
# User effect (bu)
b_u_edx <- edx %>% 
  left_join(b_i_edx, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu_edx)/(n()+lambda))
# Prediction
y_hat_edx <- validation %>% 
  left_join(b_i_edx, by = "movieId") %>%
  left_join(b_u_edx, by = "userId") %>%
  mutate(pred = mu_edx + b_i + b_u) %>%
  pull(pred)
# Update the results table
results_table <- bind_rows(results_table, 
                    tibble('Method of Analysis' = "Regularized Movie and User Effects- Final Validation", 
                           RMSE = RMSE(validation$rating, y_hat_edx)))
# Show the RMSE improvement
results_table 
```

# Conclusion

## Limitations

## Future Work


