---
title: "Practicing Machine Learning with Iris dataset"
author: "Kirtimay Pendse"
date: "6/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Required Packages

Install/require the needed packages with the following code:

```{r packages}
library("caret")
library("dslabs")
library("tidyverse")
library("dplyr")
```

## Loading the data

First we load the data, and split into train and test sets. The algorithm is developed using the train set and evaluated on the test set. We will split the Iris dataset into two, 80% of which we will use to train our models and 20% that we will hold back as a validation dataset.

```{r loading data, echo=TRUE}
data(iris)
dataset<-iris

test_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
test_set <- dataset[-test_index,]
# use the remaining 80% of data to training and testing the models
train_set <- dataset[test_index,]
```

## Exploratory analysis

We'll do the following: list the attributes of each variable, get the dataset's dimensions and levels, and look at some summary statistics.

```{r exploratory work, echo=TRUE}
# list types for each attribute
sapply(dataset, class) #except Species, every variable is numeric

dim(dataset) #we have 150 rows (observations) and 5 columns (variables)

levels(dataset$Species) #there are 3 types of species- a trinomial classification

summary(dataset)
#We can see that all of the numerical values have the same scale (centimeters) and similar ranges [0,8] cm.
```

## Visualization

To visualize the data, we'll make 2 plots- univariate plots to better understand each variable separately, and then multivariate plots to see the relationships between variables.
```{r visualization univariate, echo=TRUE}
x <- dataset[,1:4]
y <- dataset[,5]
#Given that the input variables are numeric, we can create box and whisker plots of each.

# boxplot for each attribute on one image
par(mfrow=c(1,4))
  for(i in 1:4) {
  boxplot(x[,i], main=names(iris)[i])
}
```
Let's make some multivariate plots. 

We can also look at box and whisker plots of each input variable again, but this time broken down into separate plots for each class. This can help to tease out obvious linear separations between the classes.

```{r visualization multivariate, echo=TRUE}
featurePlot(x=x, y=y, plot="box")

#This helps us see that there are clearly different distributions of the attributes for each class value
```

Next we can get an idea of the distribution of each attribute, again like the box and whisker plots, broken down by class value. Sometimes histograms are good for this, but in this case we will use some probability density plots to give nice smooth lines for each distribution.

```{r mvariate pt2, echo=TRUE}
# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)
```

We can see that features are distinct when separated by class; for example, we see that setosa (blue) has very small petal width and petal and sepal length, but has the largest sepal width on average.

## Building a few ML algorithms

Let's get into creating some models from the data and estimating their accuracy. We'll create a 10-fold cross validation algorithm using the training set, and build 5 models to predict species from flower measurements. We'll then select the best model.

We will 10-fold cross-validation to estimate accuracy.

This will split our dataset into 10 parts, train in 9 and test on 1 and release for all combinations of train-test splits. We will also repeat the process 3 times for each algorithm with different splits of the data into 10 groups, in an effort to get a more accurate estimate.

```{r algorithms, echo=TRUE}
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
```

We are using the metric of “Accuracy” to evaluate models. This is a ratio of the number of correctly predicted instances in divided by the total number of instances in the dataset multiplied by 100 to give a percentage (e.g. 95% accurate). We will be using the metric variable when we run build and evaluate each model next.

We don't know which would be the best algorithms to use on this dataset, and what configurations are optimal. From the plots we made, we can see that some of the classes are partially separable linearly, so we can expect decent results from training the following 5 models:

Linear Discriminant Analysis (LDA), Classification and Regression Trees (CART), k-Nearest Neighbors (kNN), Support Vector Machines (SVM) with a linear kernel, and Random Forest (RF).

This is a good mixture of simple linear (LDA), nonlinear (CART, kNN) and complex nonlinear methods (SVM, RF). We reset the random number seed before reach run to ensure that the evaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable.

```{r models1, echo=TRUE}
#1) Train the linear models
set.seed(7, sample.kind = "Rounding") #use set.seed(7) if using R 3.5 or earlier 
fit_lda <- train(Species~., 
                 data=dataset, 
                 method="lda", 
                 metric=metric, 
                 trControl=control)
```

```{r models2, echo=TRUE}
#2) Train the non-linear models
set.seed(7, sample.kind = "Rounding") #use set.seed(7) if using R 3.5 or earlier 
fit_cart <- train(Species~., 
                 data=dataset, 
                 method="rpart", 
                 metric=metric, 
                 trControl=control)

set.seed(7, sample.kind = "Rounding") #use set.seed(7) if using R 3.5 or earlier 
fit_knn <- train(Species~., 
                 data=dataset, 
                 method="knn", 
                 metric=metric, 
                 trControl=control)
```

```{r models3, echo=TRUE}
#3) Train the complex models
set.seed(7, sample.kind = "Rounding") #use set.seed(7) if using R 3.5 or earlier 
fit_svm <- train(Species~., 
                 data=dataset, 
                 method="svmRadial", 
                 metric=metric, 
                 trControl=control)

set.seed(7, sample.kind = "Rounding") #use set.seed(7) if using R 3.5 or earlier 
fit_rf <- train(Species~., 
                 data=dataset, 
                 method="rf", 
                 metric=metric, 
                 trControl=control)
```

## Selecting the best model

We now have 5 models and accuracy estimations for each. We need to compare the models to each other and select the most accurate. We can report on the accuracy of each model by first creating a list of the created models and using the summary function.

```{r best model, echo=TRUE}
results <- resamples(list(lda=fit_lda, cart=fit_cart, knn=fit_knn, svm=fit_svm, rf=fit_rf))
summary(results)

# compare accuracy of models using a dotplot
dotplot(results)
```

It seems like LDA is the most accurate model. We can summarize the LDA model for more details.

```{r lda, echo=TRUE}
print(fit_lda)
```
## Making a prediction

The LDA was the most accurate model. Now we want to get an idea of the accuracy of the model on our test (validation) set. This will give us an independent final check on the accuracy of the best model. It is valuable to keep a test (validation) set just in case we're overfitting to the training set or a data leak. Both will result in an overly optimistic result.

We can run the LDA model directly on the test (validation) set and summarize the results in a confusion matrix.

```{r predcitions, echo=TRUE}
predictions <- predict(fit_lda, test_set)
confusionMatrix(predictions, test_set$Species)
```


We've built a model with perfect accuracy, but our validation dataset was small (only 20%). 


